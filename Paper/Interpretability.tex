\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Interpretability}
\author{
Alina Bialkowski, Rick Wainwright, Danny Wang}
\date{February 2022}

\begin{document}

\maketitle

\section{Abstract}
As deep learning models continue to emerge, with their striking success in medical imaging diagnostics; 
an unprecedented increase in performance was shown in the speed and accuracy of the detections. Nevertheless, 
due to the black box nature of the models, the amount of trust and confidence in the diagnosis was questioned. 
In particular, accepting the outputs without any insight into the models' decision-making process could potentially 
lead to misdiagnosis. Which is a critical factor that limits the universality of such models in medical fields. In 
this paper, we present various interpretability techniques to explore the black box nature of deep learning models, 
with an attempt to gain deeper knowledge of the models' decisions and factors that could affect it. Transfer learning 
was performed on convolutional neural networks including DenseNet121 and Inception-V3 models, with the application to 
the COVIDx, ChestX-ray14 etc. datasets; publicly available datasets with thousands of labelled chest X-ray images 
comprising various diseases. Through the visualisations of saliency maps, TSNE plots and uncertainty graphs, spurious 
features have been identified in the training data. Features such as numbers, tags and devices in X-ray images provide 
no direct relation to the patients' disease, yet they were identified by the model as evidence used for classifications. 
In addition, it was also found that the datasets contained miscellaneous data resulting from the inconsistent collection 
of images. This includes duplicates, various gradients and cropped versions of the same X-ray, and incorrectly labelled 
images, which may potentially affect the trustworthiness of the predictions. However, by no means that these datasets are 
undesirable to be utilised, results have shown that even with these spurious dimensions of data exist, models were able 
to recognise similar and useful features that classified the data with its labelled class. This project was studied with 
the vision that future developments of deep learning models could improve its explainability with various interpretability 
techniques, when applied to fields that require great confidence and trust, like medical diagnosis.

\end{document}
